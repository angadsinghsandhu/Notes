{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://github.com/openai/gym\n",
        "!pip3 install autorom --upgrade\n",
        "!pip install -U ale_py==0.7.4\n",
        "!pip install gym[atari, accept-rom-license]\n",
        "!pip install gym[accept-rom-license]\n",
        "!pip install --upgrade gym==0.21.0\n",
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FvnrqxaWBpXd",
        "outputId": "a38f0b8f-fd6f-4b37-ab19-f5054c0818eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/gym\n",
            "  Cloning https://github.com/openai/gym to /tmp/pip-req-build-lbseyyn0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/gym /tmp/pip-req-build-lbseyyn0\n",
            "  Resolved https://github.com/openai/gym to commit 6a04d49722724677610e36c1f92908e72f51da0c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.26.2) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.26.2) (2.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.26.2) (6.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym==0.26.2) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym==0.26.2) (3.11.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827650 sha256=47eb69d397d41207d5619fd75241d871b78a169698f19e0835e97b40bf36538c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u4ivi6dv/wheels/fc/25/34/86b31040e1edd8970b4e245f917912b13ec6e3a648ebb5a943\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.21.0\n",
            "    Uninstalling gym-0.21.0:\n",
            "      Successfully uninstalled gym-0.21.0\n",
            "Successfully installed gym-0.26.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: autorom in /usr/local/lib/python3.8/dist-packages (0.4.2)\n",
            "Collecting autorom\n",
            "  Using cached AutoROM-0.5.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom) (7.1.2)\n",
            "Requirement already satisfied: libtorrent in /usr/local/lib/python3.8/dist-packages (from autorom) (2.0.7)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from autorom) (5.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom) (4.64.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources->autorom) (3.11.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom) (4.0.0)\n",
            "Installing collected packages: autorom\n",
            "  Attempting uninstall: autorom\n",
            "    Found existing installation: AutoROM 0.4.2\n",
            "    Uninstalling AutoROM-0.4.2:\n",
            "      Successfully uninstalled AutoROM-0.4.2\n",
            "Successfully installed autorom-0.5.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ale_py==0.7.4\n",
            "  Downloading ale_py-0.7.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from ale_py==0.7.4) (6.0.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale_py==0.7.4) (5.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from ale_py==0.7.4) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->ale_py==0.7.4) (3.11.0)\n",
            "Installing collected packages: ale_py\n",
            "  Attempting uninstall: ale_py\n",
            "    Found existing installation: ale-py 0.7.3\n",
            "    Uninstalling ale-py-0.7.3:\n",
            "      Successfully uninstalled ale-py-0.7.3\n",
            "Successfully installed ale_py-0.7.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ale_py",
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license] in /usr/local/lib/python3.8/dist-packages (0.26.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license]) (2.2.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license]) (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license]) (1.21.6)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Using cached AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (5.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.25.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (7.1.2)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (0.5.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license]) (3.11.0)\n",
            "Requirement already satisfied: libtorrent in /usr/local/lib/python3.8/dist-packages (from AutoROM.accept-rom-license->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.0.0)\n",
            "Installing collected packages: autorom\n",
            "  Attempting uninstall: autorom\n",
            "    Found existing installation: AutoROM 0.5.4\n",
            "    Uninstalling AutoROM-0.5.4:\n",
            "      Successfully uninstalled AutoROM-0.5.4\n",
            "Successfully installed autorom-0.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym==0.21.0\n",
            "  Using cached gym-0.21.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.21.0) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.21.0) (2.2.0)\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.26.2\n",
            "    Uninstalling gym-0.26.2:\n",
            "      Successfully uninstalled gym-0.26.2\n",
            "Successfully installed gym-0.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ale_py\n",
        "from ale_py import ALEInterface\n",
        "from gym import envs\n",
        "from stable_baselines3 import PPO\n",
        "import os\n",
        "from stable_baselines3.common.atari_wrappers import EpisodicLifeEnv"
      ],
      "metadata": {
        "id": "NzVtPvG1F9l-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ale = ALEInterface()\n",
        "\n",
        "print('gym:', gym.__version__)\n",
        "print('ale_py:', ale_py.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maZS862rBrFi",
        "outputId": "3d9e9f9f-445d-4f02-9ce5-b7b681db84fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gym: 0.21.0\n",
            "ale_py: 0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'ALE/Breakout-v5'\n",
        "env = gym.make(env_name)"
      ],
      "metadata": {
        "id": "aGAbWrm3B4Aa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gym Test"
      ],
      "metadata": {
        "id": "uIKOA68UF1M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs=env.reset()\n",
        "\n",
        "for step in range(int(1e3)):\n",
        "        \n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    \n",
        "    print('\\nreward: '+str(reward))\n",
        "    print('done: '  +str(done))\n",
        "\n",
        "    time.sleep(0.1)\n",
        "\n",
        "    if done:\n",
        "        \n",
        "        print('\\n\\nfinal reward:' + str(reward))\n",
        "        break\n",
        "        env.reset()\n",
        "        \n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOBBDpUpF3YL",
        "outputId": "ff07179e-a7bf-433d-a6b7-93107d3b1c9c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 1.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 1.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: True\n",
            "\n",
            "\n",
            "final reward:0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapper"
      ],
      "metadata": {
        "id": "50eb-NsxGSzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = EpisodicLifeEnv(env)\n",
        "\n",
        "state = env.reset()\n",
        "\n",
        "for step in range(int(1e3)):\n",
        "    \n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    \n",
        "    print('\\nreward: '+str(reward))\n",
        "    print('done: '  +str(done))\n",
        "\n",
        "    time.sleep(0.1)\n",
        "\n",
        "    \n",
        "    if done:\n",
        "        # it never ends...\n",
        "        print('\\n\\nfinal reward:' + str(reward))\n",
        "        break\n",
        "        env.reset()\n",
        "        \n",
        "\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxa9m6lfGUa7",
        "outputId": "2741d515-7ffb-47a0-c46c-762b9407634d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: False\n",
            "\n",
            "reward: 0.0\n",
            "done: True\n",
            "\n",
            "\n",
            "final reward:0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traning"
      ],
      "metadata": {
        "id": "BnuDPD8ZHAmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed=123\n",
        "\n",
        "#%% Training\n",
        "model = PPO(policy = 'CnnPolicy', env = env, verbose = 1, seed = seed, tensorboard_log = os.path.expanduser('~/models/breakout-v5/tb_log/'))\n",
        "\n",
        "model.learn(total_timesteps = 1e6, tb_log_name = '1.3_train')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LgEw1j3fHAPh",
        "outputId": "460c2597-7d79-4f29-8595-f6fc86a43a3e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "Logging to /root/models/breakout-v5/tb_log/1.3_train_1\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 35.8     |\n",
            "|    ep_rew_mean     | 0.228    |\n",
            "| time/              |          |\n",
            "|    fps             | 94       |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 21       |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 37.4        |\n",
            "|    ep_rew_mean          | 0.25        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 19          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 214         |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011565658 |\n",
            "|    clip_fraction        | 0.0503      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | -0.0174     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0329     |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00909    |\n",
            "|    value_loss           | 0.112       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 39.5        |\n",
            "|    ep_rew_mean          | 0.29        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 15          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 402         |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031304743 |\n",
            "|    clip_fraction        | 0.288       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.36       |\n",
            "|    explained_variance   | 0.593       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0465     |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0538     |\n",
            "|    value_loss           | 0.0163      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 43.7        |\n",
            "|    ep_rew_mean          | 0.4         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 13          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 595         |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.048153117 |\n",
            "|    clip_fraction        | 0.401       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.31       |\n",
            "|    explained_variance   | 0.769       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.111      |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.071      |\n",
            "|    value_loss           | 0.0109      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 46.1       |\n",
            "|    ep_rew_mean          | 0.48       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 12         |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 797        |\n",
            "|    total_timesteps      | 10240      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.07652604 |\n",
            "|    clip_fraction        | 0.517      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.26      |\n",
            "|    explained_variance   | 0.738      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.123     |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.0918    |\n",
            "|    value_loss           | 0.0137     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 52.7       |\n",
            "|    ep_rew_mean          | 0.65       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 12         |\n",
            "|    iterations           | 6          |\n",
            "|    time_elapsed         | 1001       |\n",
            "|    total_timesteps      | 12288      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09172887 |\n",
            "|    clip_fraction        | 0.556      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.22      |\n",
            "|    explained_variance   | 0.719      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.115     |\n",
            "|    n_updates            | 50         |\n",
            "|    policy_gradient_loss | -0.0958    |\n",
            "|    value_loss           | 0.0136     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 53.8        |\n",
            "|    ep_rew_mean          | 0.7         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 11          |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 1205        |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.110416315 |\n",
            "|    clip_fraction        | 0.569       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0.752       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.118      |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0924     |\n",
            "|    value_loss           | 0.0167      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 60.9       |\n",
            "|    ep_rew_mean          | 0.92       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 11         |\n",
            "|    iterations           | 8          |\n",
            "|    time_elapsed         | 1410       |\n",
            "|    total_timesteps      | 16384      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13475412 |\n",
            "|    clip_fraction        | 0.587      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.11      |\n",
            "|    explained_variance   | 0.659      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.12      |\n",
            "|    n_updates            | 70         |\n",
            "|    policy_gradient_loss | -0.0974    |\n",
            "|    value_loss           | 0.0168     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 62.7       |\n",
            "|    ep_rew_mean          | 0.99       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 11         |\n",
            "|    iterations           | 9          |\n",
            "|    time_elapsed         | 1612       |\n",
            "|    total_timesteps      | 18432      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.15890041 |\n",
            "|    clip_fraction        | 0.579      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.03      |\n",
            "|    explained_variance   | 0.692      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.134     |\n",
            "|    n_updates            | 80         |\n",
            "|    policy_gradient_loss | -0.0989    |\n",
            "|    value_loss           | 0.0164     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 69.3       |\n",
            "|    ep_rew_mean          | 1.16       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 11         |\n",
            "|    iterations           | 10         |\n",
            "|    time_elapsed         | 1816       |\n",
            "|    total_timesteps      | 20480      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17191324 |\n",
            "|    clip_fraction        | 0.601      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.96      |\n",
            "|    explained_variance   | 0.736      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.114     |\n",
            "|    n_updates            | 90         |\n",
            "|    policy_gradient_loss | -0.0991    |\n",
            "|    value_loss           | 0.0157     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 72.7       |\n",
            "|    ep_rew_mean          | 1.25       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 11         |\n",
            "|    iterations           | 11         |\n",
            "|    time_elapsed         | 2022       |\n",
            "|    total_timesteps      | 22528      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17721368 |\n",
            "|    clip_fraction        | 0.588      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.934     |\n",
            "|    explained_variance   | 0.643      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.11      |\n",
            "|    n_updates            | 100        |\n",
            "|    policy_gradient_loss | -0.0886    |\n",
            "|    value_loss           | 0.0228     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 75         |\n",
            "|    ep_rew_mean          | 1.35       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 11         |\n",
            "|    iterations           | 12         |\n",
            "|    time_elapsed         | 2227       |\n",
            "|    total_timesteps      | 24576      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17763132 |\n",
            "|    clip_fraction        | 0.552      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.877     |\n",
            "|    explained_variance   | 0.839      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.13      |\n",
            "|    n_updates            | 110        |\n",
            "|    policy_gradient_loss | -0.0899    |\n",
            "|    value_loss           | 0.0127     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 74.9       |\n",
            "|    ep_rew_mean          | 1.39       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 10         |\n",
            "|    iterations           | 13         |\n",
            "|    time_elapsed         | 2433       |\n",
            "|    total_timesteps      | 26624      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.21345177 |\n",
            "|    clip_fraction        | 0.604      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.867     |\n",
            "|    explained_variance   | 0.63       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.13      |\n",
            "|    n_updates            | 120        |\n",
            "|    policy_gradient_loss | -0.0921    |\n",
            "|    value_loss           | 0.0214     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 75.8       |\n",
            "|    ep_rew_mean          | 1.38       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 10         |\n",
            "|    iterations           | 14         |\n",
            "|    time_elapsed         | 2640       |\n",
            "|    total_timesteps      | 28672      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.18876381 |\n",
            "|    clip_fraction        | 0.538      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.783     |\n",
            "|    explained_variance   | 0.828      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.129     |\n",
            "|    n_updates            | 130        |\n",
            "|    policy_gradient_loss | -0.0906    |\n",
            "|    value_loss           | 0.0154     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 70.4       |\n",
            "|    ep_rew_mean          | 1.24       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 10         |\n",
            "|    iterations           | 15         |\n",
            "|    time_elapsed         | 2845       |\n",
            "|    total_timesteps      | 30720      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.26202598 |\n",
            "|    clip_fraction        | 0.602      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.84      |\n",
            "|    explained_variance   | 0.619      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.139     |\n",
            "|    n_updates            | 140        |\n",
            "|    policy_gradient_loss | -0.103     |\n",
            "|    value_loss           | 0.0208     |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 70.9      |\n",
            "|    ep_rew_mean          | 1.26      |\n",
            "| time/                   |           |\n",
            "|    fps                  | 10        |\n",
            "|    iterations           | 16        |\n",
            "|    time_elapsed         | 3049      |\n",
            "|    total_timesteps      | 32768     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.2256537 |\n",
            "|    clip_fraction        | 0.527     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.767    |\n",
            "|    explained_variance   | 0.79      |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | -0.0926   |\n",
            "|    n_updates            | 150       |\n",
            "|    policy_gradient_loss | -0.0886   |\n",
            "|    value_loss           | 0.0154    |\n",
            "---------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-53c2942c4138>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'CnnPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/models/breakout-v5/tb_log/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_log_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1.3_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    305\u001b[0m     ) -> SelfPPO:\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0;31m# Optimization step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m                 \u001b[0;31m# Clip grad norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Let's see how it plays\n",
        "obs=env.reset()\n",
        "\n",
        "for step in range(int(1e3)):\n",
        "\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    \n",
        "    time.sleep(0.1)\n",
        "\n",
        "    if done:\n",
        "        print('\\n\\nfinal reward:' + str(reward))\n",
        "        break\n",
        "        env.reset()\n",
        "        \n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOQlOwU_T_jw",
        "outputId": "7151db4c-b946-49d7-992d-3c3eafe51bb8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "final reward:0.0\n"
          ]
        }
      ]
    }
  ]
}