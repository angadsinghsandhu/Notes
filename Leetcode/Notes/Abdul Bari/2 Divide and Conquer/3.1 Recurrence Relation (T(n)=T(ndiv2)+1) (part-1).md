# 2.3.1 Recurrence Relation (T(n)=T(n/2)+1) (Part-1)

## Overview

This lecture segment delves into the analysis of recursive functions, specifically focusing on dividing functions. A dividing function is one that reduces its parameter, say `n`, by dividing it by 2 in each recursive call. This is crucial in understanding how the runtime complexity of such functions can be determined using recurrence relations.

## Dividing Functions

- **Definition**: In algorithms, dividing functions repeatedly halve their input size, representing a significant class of recursive functions.
- **Context**: Recursive functions might call themselves with a modified input, such as `n/2`, `√n`, or `n-1`. Today's focus is on functions that halve their input.

## Recurrence Relation

- **Formulation**: For a recursive algorithm that halves its input and performs a constant amount of work otherwise, we can express its time complexity as `T(n) = T(n/2) + 1`.
- **Base Case**: When `n = 1`, the recursion terminates, and `T(1) = 1`.

## Solving the Recurrence

### Using the Recursion Tree Method

- **Explanation**: Visualize the recursion as a tree where each node represents a recursive call. Each node adds a constant time cost, and each level of the tree represents a halving of the original problem size.
- **Process**:
  - Start with `T(n)`.
  - At each level of the tree, the problem size is halved, and the cost of each level is a constant `1`.
  - The depth of the tree is determined by how many times `n` can be halved until it reaches 1, which is `log₂(n)` levels.
- **Total Cost**: The sum of the costs across all levels, which is `1 + 1 + ... + 1` for `k` levels, results in `k` steps. Therefore, `T(n) = log₂(n)`.

### Using the Substitution Method

- **Explanation**: Substitute the recurrence relation into itself repeatedly to express `T(n)` in terms of smaller and smaller subproblems.
- **Process**:
  - Begin with `T(n) = T(n/2) + 1`.
  - Substitute recursively to get `T(n) = T(n/4) + 1 + 1`, and so on.
  - Continue until the subproblem size reduces to 1, which happens after `log₂(n)` substitutions.
- **Result**: Summing the constants gives `T(n) = 1 + log₂(n)`. Hence, `T(n)` is `Θ(log₂(n))`.

## Summary

This lecture introduces the method of solving recurrence relations for recursive algorithms that reduce their input size by dividing by two. Two primary methods, the recursion tree and substitution, demonstrate that the time complexity for such algorithms is `Θ(log₂(n))`, highlighting the logarithmic growth rate due to the division of the problem size. Future lectures will explore more complex recurrence relations and introduce the Master Theorem for broader applicative scenarios.
