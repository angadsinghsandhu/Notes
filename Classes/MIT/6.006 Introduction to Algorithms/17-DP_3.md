# 17. Dynamic Programming, Part 3: APSP, Parenthesization, Piano

## Overview

This lecture focuses on **subproblem constraints and expansion**—the technique of adding parameters to subproblems to "remember state" and enable recurrences.

Examples covered:
1. **Bellman-Ford** as DP (review) - constraint on number of edges
2. **Floyd-Warshall** - all-pairs shortest paths in O(V³)
3. **Arithmetic Parenthesization** - optimizing expression evaluation
4. **Piano/Guitar Fingering** - optimal finger assignments

Key insight: By adding constraints/parameters to subproblems, we can track additional state needed to write correct recurrences.

---

## Historical Note: Why "Dynamic Programming"?

Richard Bellman invented DP in the 1950s. The name was chosen because:
- **Programming** = optimization (old usage, like "linear programming")
- **Dynamic** = decisions made stage-by-stage, adapting locally

Mostly, it just sounded impressive for grant proposals!

---

## SRTBOT Review with New Insights

| Step | Key Lesson |
|------|------------|
| **S** - Subproblems | Prefixes/suffixes/substrings for sequences. **Can always add subproblems** to make relations easier. Adding constraints = "remembering state." |
| **R** - Relations | Identify a **question** about the solution. If you knew the answer, you could reduce to smaller subproblems. **Locally brute force** all possible answers. |
| **T** - Topological Order | Ensure acyclicity. Usually nested for loops. |
| **B** - Base Cases | Smallest subproblems with known answers. |
| **O** - Original Problem | Solve in terms of subproblems. May need to try multiple subproblems. |
| **T** - Time | Σ(work per subproblem) ≤ (# subproblems) × (max work each) + (original problem work) |

### The "Guessing" Perspective

Another way to think about local brute force:
1. Identify a question about the solution (e.g., "What is the last edge?")
2. **Guess** the correct answer
3. Assume the guess is correct, recurse
4. Since we don't actually know, **try all guesses** and take the best

---

## Example 1: Bellman-Ford as Dynamic Programming

### DAG Shortest Paths (Review)

For a DAG, we can write:

```
δ(s, v) = min over incoming edges (u,v): δ(s, u) + w(u, v)
```

**Question being answered**: What is the last edge (u, v) on a shortest s→v path?

This works because the graph is acyclic—the subproblem graph matches G, which is a DAG.

**Problem**: If G has cycles, this recurrence has cycles → infinite recursion.

### Adding a Constraint: Edge Count

**Key idea**: Add a constraint to break cycles.

```
δₖ(s, v) = weight of shortest s→v path using at most k edges
```

This is **subproblem expansion**—we multiply subproblems by V (choices for k).

### SRTBOT for Bellman-Ford

```
Subproblems:    δₖ(s, v) = shortest path s→v using ≤ k edges
                for k ∈ {0, ..., |V|-1}, v ∈ V
                O(V²) subproblems

Relation:       δₖ(s, v) = min(
                    δₖ₋₁(s, v),                           # Use < k edges
                    min over (u,v) ∈ E: δₖ₋₁(s, u) + w(u,v)  # Use exactly k edges
                )

Topological:    Increasing k (k = 0, 1, ..., |V|-1)
                All references are to k-1, so acyclic!

Base Cases:     δ₀(s, s) = 0
                δ₀(s, v) = ∞ for v ≠ s

Original:       δ|V|-1(s, v) for all v
                (Simple paths have at most |V|-1 edges)

Time:           Σᵥ Σₖ O(in-degree(v)) = O(VE)
```

### Why This Works

The constraint k **breaks cycles** in the subproblem graph:
- Without k: δ(s, v) depends on δ(s, u) which might depend on δ(s, v) → cycle!
- With k: δₖ(s, v) only depends on δₖ₋₁(s, ·) → always acyclic

```
Original graph G may have cycles:
    u ←→ v

Subproblem graph is acyclic:
    δ₀(s,u) → δ₁(s,v) → δ₂(s,u) → δ₃(s,v) → ...
```

---

## Example 2: Floyd-Warshall (All-Pairs Shortest Paths)

### Problem

Compute δ(u, v) for ALL pairs u, v ∈ V.

### Known Solutions

| Algorithm | Time | Best For |
|-----------|------|----------|
| Bellman-Ford × V | O(V²E) = O(V⁴) dense | - |
| Johnson's | O(VE + V² log V) | Sparse graphs |
| Floyd-Warshall | O(V³) | Dense graphs, simplicity |

### The Floyd-Warshall Idea

Instead of constraining by **edge count**, constrain by **which vertices** can be used.

**Setup**: Number vertices 1, 2, ..., |V|

```
d(u, v, k) = weight of shortest u→v path using only vertices 
             from {u, v} ∪ {1, 2, ..., k} as intermediates
```

### Why This Constraint?

- k = 0: Only direct edges allowed (no intermediate vertices)
- k = 1: Can use vertex 1 as intermediate
- k = 2: Can use vertices 1 and 2 as intermediates
- ...
- k = |V|: Can use any vertex → regular shortest path!

### SRTBOT for Floyd-Warshall

```
Subproblems:    d(u, v, k) = shortest u→v path using intermediates ⊆ {1,...,k}
                for u, v ∈ V, k ∈ {0, ..., |V|}
                O(V³) subproblems

Relation:       d(u, v, k) = min(
                    d(u, v, k-1),                      # Don't use vertex k
                    d(u, k, k-1) + d(k, v, k-1)        # Use vertex k
                )

Topological:    for k = 0 to |V|:
                    for u = 1 to |V|:
                        for v = 1 to |V|:
                            compute d(u, v, k)

Base Cases:     d(u, v, 0) = 
                    0           if u = v
                    w(u, v)     if edge (u,v) exists
                    ∞           otherwise

Original:       d(u, v, |V|) for all u, v

Time:           O(V³) subproblems × O(1) work = O(V³)
```

### Understanding the Relation

**Question**: Does the shortest u→v path (using vertices 1..k) go through vertex k?

**Two cases**:
1. **No**: Best path uses only vertices 1..k-1 → d(u, v, k-1)
2. **Yes**: Path goes u → ... → k → ... → v
   - First part: u to k using vertices 1..k-1
   - Second part: k to v using vertices 1..k-1
   - Total: d(u, k, k-1) + d(k, v, k-1)

```
Case 1: Path avoids k          Case 2: Path uses k
                               
u --------→ v                  u ----→ k ----→ v
  (uses 1..k-1)                  (1..k-1) (1..k-1)
```

### Why Only O(1) Work Per Subproblem?

Unlike Bellman-Ford where we loop over all incoming edges, here we only consider **two options**:
- Use vertex k or don't use vertex k

This is the key insight that reduces O(V⁴) to O(V³).

### Implementation

```python
def floyd_warshall(n, edges):
    # Initialize distance matrix
    INF = float('inf')
    d = [[INF] * n for _ in range(n)]
    
    # Base case: k = 0
    for u in range(n):
        d[u][u] = 0
    for u, v, w in edges:
        d[u][v] = w
    
    # DP: increasing k
    for k in range(n):
        for u in range(n):
            for v in range(n):
                if d[u][k] + d[k][v] < d[u][v]:
                    d[u][v] = d[u][k] + d[k][v]
    
    return d
```

### Comparison: When to Use What?

| Graph Type | E | Johnson's | Floyd-Warshall |
|------------|---|-----------|----------------|
| Very sparse | Θ(V) | O(V² log V) ✓ | O(V³) |
| Moderate | Θ(V^1.5) | O(V^2.5) ✓ | O(V³) |
| Dense | Θ(V²) | O(V³) | O(V³) ✓ (simpler) |

Floyd-Warshall wins on simplicity—just 5 lines of code!

---

## Example 3: Arithmetic Parenthesization

### Problem Definition

Given a formula with + and × operators:
```
a₀ ★₁ a₁ ★₂ a₂ ... ★ₙ₋₁ aₙ₋₁
```
where each aᵢ is an integer and each ★ᵢ is + or ×.

**Goal**: Place parentheses to **maximize** (or minimize) the result.

### Example

```
Formula: 7 + 4 × 3 + 5

Standard precedence: 7 + (4 × 3) + 5 = 7 + 12 + 5 = 24

Optimal parenthesization: (7 + 4) × (3 + 5) = 11 × 8 = 88 ✓
```

### Why Substrings?

**Question to ask**: Which operation is evaluated **last** (at the root of expression tree)?

If we guess the last operation is ★ₖ:
- Left subtree = everything left of ★ₖ (a substring)
- Right subtree = everything right of ★ₖ (a substring)

```
Expression: a₀ ★₁ a₁ ★₂ a₂ ★₃ a₃

If ★₂ is last:
        ★₂
       /  \
   [a₀★₁a₁] [a₂★₃a₃]
   (substring) (substring)
```

**Key insight**: A prefix of a suffix = a substring. So we need substrings, not just prefixes/suffixes.

### The Negative Number Complication

With positive numbers only:
- To maximize a sum: maximize both parts
- To maximize a product: maximize both parts

With negative numbers:
```
Formula: 7 + (-4) × 3 + (-5)

Naive: (7 + (-4)) × (3 + (-5)) = 3 × (-2) = -6  ✗

Better: 7 + ((-4) × 3 + (-5)) = 7 + (-12 + (-5)) = 7 + (-17) = -10  
        Still negative...

Best: 7 + ((-4) × (3 + (-5))) = 7 + ((-4) × (-2)) = 7 + 8 = 15  ✓
```

**Problem**: To maximize a product, sometimes we want to **minimize** the parts (to get two negative numbers whose product is positive).

**Solution**: Solve both MAX and MIN for every subproblem.

### SRTBOT Solution

```
Subproblems:    x(i, j, opt) = the opt value achievable for substring aᵢ ★ᵢ₊₁ ... ★ⱼ₋₁ aⱼ₋₁
                where opt ∈ {min, max}
                for 0 ≤ i < j ≤ n
                O(n²) × 2 = O(n²) subproblems

Relation:       x(i, j, opt) = opt over k ∈ (i, j), optL ∈ {min,max}, optR ∈ {min,max}:
                    x(i, k, optL) ★ₖ x(k, j, optR)
                
                where ★ₖ is the k-th operator (+ or ×)

Topological:    Increasing (j - i) — increasing substring length
                for length = 1 to n:
                    for i = 0 to n - length:
                        j = i + length
                        compute x(i, j, min) and x(i, j, max)

Base Cases:     x(i, i+1, opt) = aᵢ    (single number, no choice)

Original:       x(0, n, max)

Time:           O(n²) subproblems × O(n) work = O(n³)
```

### Understanding the Relation

**Question**: Which operator ★ₖ is evaluated last?

For each choice of k:
- Left part: x(i, k, optL)
- Right part: x(k, j, optR)
- Combine: left ★ₖ right

**Why try all combinations of optL and optR?**

| Operator | To Maximize | To Minimize |
|----------|-------------|-------------|
| + | max + max | min + min |
| × (both positive) | max × max | min × min |
| × (both negative) | min × min | max × max |
| × (mixed signs) | complicated... | complicated... |

Rather than case analysis, just try all 4 combinations and take the best!

### Implementation

```python
def max_parenthesization(a, ops):
    """
    a: list of integers
    ops: list of operators ('+' or '*'), length = len(a) - 1
    """
    n = len(a)
    INF = float('inf')
    
    # x[i][j][0] = min value, x[i][j][1] = max value
    x = [[[INF, -INF] for _ in range(n + 1)] for _ in range(n + 1)]
    
    # Base cases: single numbers
    for i in range(n):
        x[i][i + 1][0] = a[i]  # min
        x[i][i + 1][1] = a[i]  # max
    
    # Fill by increasing length
    for length in range(2, n + 1):
        for i in range(n - length + 1):
            j = i + length
            for k in range(i + 1, j):  # k is the last operator position
                op = ops[k - 1]  # operator between a[k-1] and a[k]
                
                # Try all combinations of min/max for left and right
                for opt_l in [0, 1]:
                    for opt_r in [0, 1]:
                        left = x[i][k][opt_l]
                        right = x[k][j][opt_r]
                        
                        if op == '+':
                            val = left + right
                        else:  # op == '*'
                            val = left * right
                        
                        x[i][j][0] = min(x[i][j][0], val)  # update min
                        x[i][j][1] = max(x[i][j][1], val)  # update max
    
    return x[0][n][1]  # max value for entire expression
```

### Extension: Other Operators

| Operator | Works with min/max? |
|----------|---------------------|
| + | ✓ Yes |
| × | ✓ Yes (with both min and max) |
| - | ✓ Yes (subtraction = add negative) |
| ÷ | ⚠️ Need to track "closest to 0" as well |

For division, the optimal might require values close to 0 (not just min/max), so you'd need 4 quantities per subproblem instead of 2.

---

## Example 4: Piano/Guitar Fingering

### Problem Definition

**Given**: 
- Sequence of notes: t₀, t₁, ..., tₙ₋₁
- F fingers (typically 5 per hand)
- Difficulty function: d(t, f, t', f') = difficulty of playing note t with finger f, then note t' with finger f'

**Goal**: Assign fingers f₀, f₁, ..., fₙ₋₁ to minimize total difficulty:
```
minimize Σᵢ d(tᵢ, fᵢ, tᵢ₊₁, fᵢ₊₁)
```

### Difficulty Function Examples

The difficulty function d(t, f, t', f') captures:
- **Stretch penalty**: Large interval + small finger span = hard
- **Weak finger penalty**: Using 4th/5th fingers
- **Crossing penalty**: Going from high finger to low finger for ascending notes
- **Legato penalty**: Same finger for consecutive different notes (can't sustain)
- **Position shift**: Moving hand position

### Why We Need Subproblem Expansion

**Naive subproblems**: x(i) = min difficulty to play suffix tᵢ, ..., tₙ₋₁

**Problem**: To compute difficulty d(tᵢ, fᵢ, tᵢ₊₁, fᵢ₊₁), we need to know BOTH:
- fᵢ (finger for current note)
- fᵢ₊₁ (finger for next note)

But if we just recurse on x(i+1), we don't know what finger was chosen for tᵢ₊₁!

**Solution**: Add finger as a parameter.

### SRTBOT Solution

```
Subproblems:    x(i, f) = min difficulty to play suffix tᵢ, ..., tₙ₋₁
                          starting with finger f on note tᵢ
                for i ∈ {0, ..., n}, f ∈ {1, ..., F}
                O(nF) subproblems

Relation:       x(i, f) = min over f' ∈ {1, ..., F}:
                    d(tᵢ, f, tᵢ₊₁, f') + x(i+1, f')
                
                (Guess which finger f' to use for next note)

Topological:    Decreasing i (suffix order)
                for i = n-1 down to 0:
                    for f = 1 to F:
                        compute x(i, f)

Base Cases:     x(n, f) = 0 for all f    (no more notes)
                or x(n-1, f) = 0         (last note, no transition)

Original:       min over f ∈ {1, ..., F}: x(0, f)
                (Don't know which finger to start with, try all)

Time:           O(nF) subproblems × O(F) work = O(nF²)
                For F = 5 (constant): O(n) — linear time!
```

### Subproblem DAG Visualization

```
Original: min over starting fingers
    │
    ├──→ x(0,1) ──→ x(1,1) ──→ x(2,1) ──→ ... ──→ x(n-1,1) ──→ 0
    │      │╲        │╲        │╲
    ├──→ x(0,2) ──→ x(1,2) ──→ x(2,2) ──→ ... ──→ x(n-1,2) ──→ 0
    │      │ ╲       │ ╲       │ ╲
    ├──→ x(0,3) ──→ x(1,3) ──→ x(2,3) ──→ ... ──→ x(n-1,3) ──→ 0
    │      │  ╲      │  ╲      │  ╲
    ├──→ x(0,4) ──→ x(1,4) ──→ x(2,4) ──→ ... ──→ x(n-1,4) ──→ 0
    │      │   ╲     │   ╲     │   ╲
    └──→ x(0,5) ──→ x(1,5) ──→ x(2,5) ──→ ... ──→ x(n-1,5) ──→ 0

Each x(i,f) has edges to ALL x(i+1,f') with weight d(tᵢ,f,tᵢ₊₁,f')
This is shortest path in a DAG!
```

### Implementation

```python
def piano_fingering(notes, difficulty, F=5):
    """
    notes: list of notes to play
    difficulty: function d(t, f, t', f') -> cost
    F: number of fingers
    """
    n = len(notes)
    if n == 0:
        return 0, []
    
    INF = float('inf')
    
    # x[i][f] = min difficulty for suffix starting at i with finger f
    x = [[INF] * (F + 1) for _ in range(n + 1)]
    parent = [[None] * (F + 1) for _ in range(n)]
    
    # Base case
    for f in range(1, F + 1):
        x[n][f] = 0
    
    # Fill backwards
    for i in range(n - 1, -1, -1):
        for f in range(1, F + 1):
            if i == n - 1:
                x[i][f] = 0  # Last note, no transition cost
            else:
                for f_next in range(1, F + 1):
                    cost = difficulty(notes[i], f, notes[i + 1], f_next) + x[i + 1][f_next]
                    if cost < x[i][f]:
                        x[i][f] = cost
                        parent[i][f] = f_next
    
    # Find best starting finger
    best_f = min(range(1, F + 1), key=lambda f: x[0][f])
    
    # Reconstruct fingering
    fingering = []
    f = best_f
    for i in range(n):
        fingering.append(f)
        if i < n - 1:
            f = parent[i][f]
    
    return x[0][best_f], fingering
```

### Extensions

**Multiple notes at once (chords)**:
- State = which fingers are on which notes
- Up to F notes at once → up to T^F states (T = max notes at once)
- Time: O(n × T^(2F)) — still polynomial if T, F are constants

**Guitar fingering**:
- Same note can be played on different strings
- Add string choice to the state
- Difficulty includes fret positions, string crossings, etc.

**Two hands**:
- Either solve separately (if hand assignment is known)
- Or expand state to include both hands' finger positions

---

## Key Concept: Subproblem Expansion

### When to Expand

Expand subproblems when you **can't write a recurrence** because:
1. You need information about "the past" (what came before)
2. You need to track some "state" that affects future decisions
3. The obvious subproblems create cycles

### How to Expand

Add a parameter that captures the needed information:

| Problem | Expansion | Factor |
|---------|-----------|--------|
| Bellman-Ford | Edge count k | ×V |
| Floyd-Warshall | Vertex set {1..k} | ×V |
| Coin Game | Current player | ×2 |
| Parenthesization | min vs max | ×2 |
| Piano Fingering | Current finger | ×F |

### The Trade-off

More parameters = more subproblems = potentially slower

But: More parameters = easier recurrences = actually works!

**Rule of thumb**: Add parameters until you can write a correct recurrence, but keep the total subproblem count polynomial.

---

## Summary

| Problem | Subproblems | Expansion | Time |
|---------|-------------|-----------|------|
| **Bellman-Ford** | δₖ(s,v) | Edge count k | O(VE) |
| **Floyd-Warshall** | d(u,v,k) | Vertex prefix {1..k} | O(V³) |
| **Parenthesization** | x(i,j,opt) | min/max | O(n³) |
| **Piano Fingering** | x(i,f) | Starting finger | O(nF²) |

### Key Takeaways

1. **Subproblem expansion** = adding parameters to track state
2. **Constraints break cycles**: k edges, k vertices, etc.
3. **When stuck**: Ask "what information would let me write a recurrence?"
4. **Brute force is fine**: Try all options for unknown quantities (optL, optR, next finger, etc.)
5. **Many DPs reduce to shortest paths** in a DAG, but writing DP directly is often simpler

---

## Next Lecture

- More DP examples
- Pseudopolynomial time algorithms
- Knapsack problem
